{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Future/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "LOCAL_LOCATION_X = \"../Data/fpkm_normalized.csv\"\n",
    "LOCAL_LOCATION_Y = \"../Data/optimal_encoded_scae_dropout.csv\"\n",
    "DAMAVAND_LOCATION_X = \"~/f/Behrooz/dataset_local/fpkm_normalized.csv\"\n",
    "DAMAVAND_LOCATION_Y = \"~/f/Behrooz/dataset_local/optimal_encoded_scae_dropout.csv\"\n",
    "\n",
    "# Hyper-Parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "DROP_OUT = 0.5\n",
    "N_SAMPLES = 10787\n",
    "N_FEATURES = 19671\n",
    "N_DISEASES = 34\n",
    "N_BATCHES = 2000\n",
    "N_EPOCHS = 100\n",
    "N_BATCH_LEARN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_initializer(shape, stddev=0.01, name=None):\n",
    "    initial = tf.random_normal(shape=shape, stddev=stddev)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_initializer(shape, init_value=0.1, name=None):\n",
    "    initial = tf.constant(init_value, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def load_data(filename):  # TODO search for faster way to load data\n",
    "    return pd.read_csv(filename, header=None)\n",
    "\n",
    "\n",
    "def fully_connected(input_data, weight, bias, name=None):\n",
    "    return tf.nn.relu(tf.add(tf.matmul(input_data, weight), bias, name=name))\n",
    "\n",
    "\n",
    "def drop_out(prev_output, keep_prob):\n",
    "    return tf.nn.dropout(prev_output, keep_prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_data, y_data, steps=5000, batch_size=20, num_epochs=10000):\n",
    "    # Split data into train/test = 80%/20%\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size=0.20)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # training_size = X_train.shape[0]\n",
    "\n",
    "    # Create Network and Variables\n",
    "    with tf.Graph().as_default():\n",
    "        feature_columns = [tf.feature_column.numeric_column('x', shape=X_train.shape[1:])]\n",
    "        regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_columns,\n",
    "                                                  activation_fn=tf.nn.relu, hidden_units=[1024, 256, 64])\n",
    "\n",
    "        def input_fn(x, y=None):\n",
    "            if y is not None:  # Training\n",
    "                features = {k: tf.constant(x[k].values) for k in x.columns}\n",
    "                responses = tf.constant(y.values, shape=y.shape)\n",
    "                return features, responses\n",
    "            else:  # Testing\n",
    "                features = {k: tf.constant(x[k].values) for k in x_data.columns}\n",
    "                return features\n",
    "\n",
    "        # Training...\n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'x': X_train.values}, y=Y_train.values, batch_size=batch_size, num_epochs=num_epochs, shuffle=True)\n",
    "        print(\"Training...\")\n",
    "        regressor.fit(input_fn=train_input_fn, steps=steps)\n",
    "\n",
    "        print(\"Testing...\")\n",
    "        test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'x': X_test.values}, y=Y_test.values, num_epochs=1, shuffle=False)\n",
    "        predictions = regressor.predict_scores(input_fn=test_input_fn)\n",
    "\n",
    "        predictions = [p for p in predictions]\n",
    "#         print(predictions)\n",
    "#         print(len(predictions))\n",
    "        y_predicted = np.array(predictions)\n",
    "        y_predicted = y_predicted.reshape(np.array(Y_test).shape)\n",
    "\n",
    "        # Score with sklearn\n",
    "        score_sklearn = metrics.mean_squared_error(Y_test, y_predicted)\n",
    "        print('MSE (sklearn): {0:f}'.format(score_sklearn))\n",
    "\n",
    "        # Score with tensorflow\n",
    "        scores = regressor.evaluate(input_fn=test_input_fn)\n",
    "        print('MSE (tensorflow): {0:f}'.format(scores['loss']))\n",
    "\n",
    "        # regressor.fit(input_fn=lambda: input_fn(X_train, Y_train), steps=100)\n",
    "        # evaluation = regressor.evaluate(input_fn=lambda: input_fn(X_test), steps=1)\n",
    "        # loss_score = evaluation[\"loss\"]\n",
    "        # print(\"Final Loss on the testing set: {0:f}\".format(loss_score))\n",
    "        # y = regressor.predict(input_fn=lambda: input_fn(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x_data, y_data, n_features=50, n_diseases=1, n_epochs=250, n_batches=1000, n_batch_learn=10):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size=0.20)\n",
    "    training_size = X_train.shape[0]\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        x = tf.placeholder(tf.float32, shape=[None, n_features])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, n_diseases])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        neurons = {  # TODO train This new architecture\n",
    "            'in': n_features,\n",
    "            'l1': 1024,\n",
    "            'l2': 256,\n",
    "            'l3': 64,\n",
    "            'out': n_diseases\n",
    "        }\n",
    "        weights = {\n",
    "            'l1': weight_initializer(shape=[neurons['in'], neurons['l1']], stddev=0.1, name='w1'),\n",
    "            'l2': weight_initializer(shape=[neurons['l1'], neurons['l2']], stddev=0.1, name='w2'),\n",
    "            'l3': weight_initializer(shape=[neurons['l2'], neurons['l3']], stddev=0.1, name='w3'),\n",
    "            'out': weight_initializer(shape=[neurons['l3'], neurons['out']], stddev=0.1, name='w_out')\n",
    "        }\n",
    "\n",
    "        biases = {\n",
    "            'l1': bias_initializer(init_value=0.1, shape=[neurons['l1']], name='b1'),\n",
    "            'l2': bias_initializer(init_value=0.1, shape=[neurons['l2']], name='b2'),\n",
    "            'l3': bias_initializer(init_value=0.1, shape=[neurons['l3']], name='b3'),\n",
    "            'out': bias_initializer(init_value=0.1, shape=[neurons['out']], name='b_out')\n",
    "        }\n",
    "        # 1st Layer --> Fully Connected (1024 Neurons)\n",
    "        layer_1 = fully_connected(x, weights['l1'], biases['l1'], name='l1')\n",
    "        layer_1 = drop_out(layer_1, keep_prob)\n",
    "\n",
    "        # 2nd Layer --> Fully Connected (256 Neurons)\n",
    "        layer_2 = fully_connected(layer_1, weights['l2'], biases['l2'], name='l2')\n",
    "        layer_2 = drop_out(layer_2, keep_prob)\n",
    "\n",
    "        # 3rd Layer --> Fully Connected (64 Neurons)\n",
    "        layer_3 = fully_connected(layer_2, weights['l3'], biases['l3'], name='l3')\n",
    "        layer_3 = drop_out(layer_3, keep_prob)\n",
    "\n",
    "        # Final Layer --> Fully Connected (N_DISEASES Neurons)\n",
    "        final_output = fully_connected(layer_3, weights['out'], biases['out'], name='l_out')\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(final_output - y),name='loss')\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001, name='optimizer')\n",
    "        train_step = optimizer.minimize(loss, name='train_step')\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        training_acc = []\n",
    "        validation_acc = []\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            \n",
    "            for epoch in range(n_epochs):\n",
    "                # Train Network\n",
    "                for i in range(n_batch_learn):\n",
    "                    batch_indices = np.random.choice(training_size, size=n_batches)\n",
    "                    x_train_batch = X_train.iloc[batch_indices]\n",
    "                    y_train_batch = Y_train.iloc[batch_indices]\n",
    "\n",
    "                    feed_dict = {x: x_train_batch, y: y_train_batch, keep_prob: DROP_OUT}\n",
    "                    _, train_loss = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "                    training_loss.append(train_loss)\n",
    "                    prediction = tf.nn.softmax(final_output)\n",
    "                    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_train_batch, 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                    feed_dict = {x: x_train_batch, y: y_train_batch, keep_prob: 1.0}\n",
    "                    training_acc.append(accuracy.eval(feed_dict))\n",
    "\n",
    "                    # Test Validation set\n",
    "                    feed_dict = {x: X_test, y: Y_test, keep_prob: 1.0}\n",
    "                    validation_loss.append(sess.run(loss, feed_dict=feed_dict))\n",
    "                    prediction = tf.nn.softmax(final_output)\n",
    "                    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y_test, 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                    validation_acc.append(accuracy.eval(feed_dict))\n",
    "                if epoch % 5 == 0:\n",
    "                    print(\"Epoch:\", '%04d' % (epoch + 1) if epoch == 0 else epoch,\n",
    "    #                       \"\\tValidation Accuracy =\", '%01.9f' % (validation_acc[-1]),\n",
    "                          \"\\tValidation Loss =\", '%09.5f' % (validation_loss[-1]),\n",
    "    #                       \"\\tTraining Accuracy =\", '%01.9f' % (training_acc[-1]),\n",
    "                          \"\\tTraining Loss =\", '%09.5f' % (training_loss[-1]))\n",
    "            \n",
    "        print(\"Training Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6754  7657 15877  9436 18254  4932  9344 14987  5105 15256 13168  4582\n",
      "  3842 17394  7482  8262  1403 12413  7309   274 15144  9325  1888 12452\n",
      "  7157  6334  9319  8696  3284  1485  5550  7127  1873  3417  5126 10667\n",
      "  3814  5438  8813 13760  5489 17299   331    98  5192  7813 16951 19005\n",
      " 13334 15886]\n",
      "\n",
      "Loding Data...\n",
      "(10787,)\n",
      "Train Deep Neural Networks\n"
     ]
    }
   ],
   "source": [
    "random_features = np.random.choice(19671, 50, replace=False)\n",
    "print(np.array(random_features))\n",
    "print(\"\\nLoding Data...\")\n",
    "# x_data = pd.DataFrame([[i for i in range(50)] for _ in range(100)])\n",
    "# y_data = pd.DataFrame([[i] for i in range(100)])\n",
    "x_data, y_data = load_data(LOCAL_LOCATION_X)[random_features], load_data(LOCAL_LOCATION_Y)[0]\n",
    "y_data = pd.DataFrame(np.reshape(y_data, newshape=[-1, 1]))\n",
    "print(y_data.shape)\n",
    "print(\"Train Deep Neural Networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# train(x_data, y_data, num_epochs=1000, steps=50000, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 \tValidation Loss = 000.27970 \tTraining Loss = 000.35274\n",
      "Epoch: 0006 \tValidation Loss = 000.17299 \tTraining Loss = 000.17328\n",
      "Epoch: 0011 \tValidation Loss = 000.14423 \tTraining Loss = 000.15462\n",
      "Epoch: 0016 \tValidation Loss = 000.11863 \tTraining Loss = 000.13050\n",
      "Epoch: 0021 \tValidation Loss = 000.09726 \tTraining Loss = 000.11560\n",
      "Epoch: 0026 \tValidation Loss = 000.08760 \tTraining Loss = 000.09505\n",
      "Epoch: 0031 \tValidation Loss = 000.08294 \tTraining Loss = 000.08174\n",
      "Epoch: 0036 \tValidation Loss = 000.07741 \tTraining Loss = 000.06837\n",
      "Epoch: 0041 \tValidation Loss = 000.07415 \tTraining Loss = 000.07110\n",
      "Epoch: 0046 \tValidation Loss = 000.07317 \tTraining Loss = 000.05958\n",
      "Epoch: 0051 \tValidation Loss = 000.07225 \tTraining Loss = 000.05955\n",
      "Epoch: 0056 \tValidation Loss = 000.07056 \tTraining Loss = 000.05464\n",
      "Epoch: 0061 \tValidation Loss = 000.06979 \tTraining Loss = 000.04511\n",
      "Epoch: 0066 \tValidation Loss = 000.06873 \tTraining Loss = 000.04559\n",
      "Epoch: 0071 \tValidation Loss = 000.07078 \tTraining Loss = 000.05279\n",
      "Epoch: 0076 \tValidation Loss = 000.07241 \tTraining Loss = 000.04523\n",
      "Epoch: 0081 \tValidation Loss = 000.06968 \tTraining Loss = 000.04054\n",
      "Epoch: 0086 \tValidation Loss = 000.07119 \tTraining Loss = 000.03386\n",
      "Epoch: 0091 \tValidation Loss = 000.07117 \tTraining Loss = 000.03671\n",
      "Epoch: 0096 \tValidation Loss = 000.06887 \tTraining Loss = 000.03051\n",
      "Epoch: 0101 \tValidation Loss = 000.06723 \tTraining Loss = 000.03126\n",
      "Epoch: 0106 \tValidation Loss = 000.07094 \tTraining Loss = 000.03126\n",
      "Epoch: 0111 \tValidation Loss = 000.06994 \tTraining Loss = 000.02915\n",
      "Epoch: 0116 \tValidation Loss = 000.07154 \tTraining Loss = 000.02372\n",
      "Epoch: 0121 \tValidation Loss = 000.07012 \tTraining Loss = 000.02144\n",
      "Epoch: 0126 \tValidation Loss = 000.07071 \tTraining Loss = 000.02006\n",
      "Epoch: 0131 \tValidation Loss = 000.07132 \tTraining Loss = 000.01551\n",
      "Epoch: 0136 \tValidation Loss = 000.07209 \tTraining Loss = 000.02258\n",
      "Epoch: 0141 \tValidation Loss = 000.06942 \tTraining Loss = 000.01772\n",
      "Epoch: 0146 \tValidation Loss = 000.07392 \tTraining Loss = 000.01641\n",
      "Epoch: 0151 \tValidation Loss = 000.07499 \tTraining Loss = 000.02122\n",
      "Epoch: 0156 \tValidation Loss = 000.07358 \tTraining Loss = 000.01669\n",
      "Epoch: 0161 \tValidation Loss = 000.07276 \tTraining Loss = 000.01958\n",
      "Epoch: 0166 \tValidation Loss = 000.07481 \tTraining Loss = 000.01614\n"
     ]
    }
   ],
   "source": [
    "model_2(x_data, y_data, n_epochs=250, n_batches=1000, n_batch_learn=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
