{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Future/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "LOCAL_LOCATION_X = \"../Data/fpkm_normalized.csv\"\n",
    "LOCAL_LOCATION_Y = \"../Data/optimal_encoded_scae_dropout.csv\"\n",
    "\n",
    "# Hyper-Parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "DROP_OUT = 0.5\n",
    "N_SAMPLES = 10787\n",
    "N_FEATURES = 19671\n",
    "N_DISEASES = 34\n",
    "N_BATCHES = 2000\n",
    "N_EPOCHS = 100\n",
    "N_BATCH_LEARN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_initializer(shape, stddev=0.01, name=None):\n",
    "    initial = tf.random_normal(shape=shape, stddev=stddev)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_initializer(shape, init_value=0.1, name=None):\n",
    "    initial = tf.constant(init_value, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def load_data(filename):  # TODO search for faster way to load data\n",
    "    return pd.read_csv(filename, header=None)\n",
    "\n",
    "\n",
    "def fully_connected(input_data, weight, bias, name=None):\n",
    "    return tf.nn.relu(tf.add(tf.matmul(input_data, weight), bias, name=name))\n",
    "\n",
    "\n",
    "def drop_out(prev_output, keep_prob):\n",
    "    return tf.nn.dropout(prev_output, keep_prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_data, y_data):\n",
    "    # Split data into train/test = 80%/20%\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size=0.20)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # training_size = X_train.shape[0]\n",
    "\n",
    "    # Create Network and Variables\n",
    "    with tf.Graph().as_default():\n",
    "        feature_columns = [tf.feature_column.numeric_column('x', shape=X_train.shape[1:])]\n",
    "        regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_columns,\n",
    "                                                  activation_fn=tf.nn.relu, hidden_units=[1024, 256, 64])\n",
    "\n",
    "        def input_fn(x, y=None):\n",
    "            if y is not None:  # Training\n",
    "                features = {k: tf.constant(x[k].values) for k in x.columns}\n",
    "                responses = tf.constant(y.values, shape=y.shape)\n",
    "                return features, responses\n",
    "            else:  # Testing\n",
    "                features = {k: tf.constant(x[k].values) for k in x_data.columns}\n",
    "                return features\n",
    "\n",
    "        # Training...\n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'x': X_train.values}, y=Y_train.values, batch_size=50, num_epochs=10000, shuffle=True)\n",
    "        print(\"Training...\")\n",
    "        regressor.fit(input_fn=train_input_fn, steps=5000)\n",
    "\n",
    "        print(\"Testing...\")\n",
    "        test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'x': X_test.values}, y=Y_test.values, num_epochs=1, shuffle=False)\n",
    "        predictions = regressor.predict_scores(input_fn=test_input_fn)\n",
    "\n",
    "        predictions = [p for p in predictions]\n",
    "        print(predictions)\n",
    "        print(len(predictions))\n",
    "        y_predicted = np.array(predictions)\n",
    "        y_predicted = y_predicted.reshape(np.array(Y_test).shape)\n",
    "\n",
    "        # Score with sklearn\n",
    "        score_sklearn = metrics.mean_squared_error(Y_test, y_predicted)\n",
    "        print('MSE (sklearn): {0:f}'.format(score_sklearn))\n",
    "\n",
    "        # Score with tensorflow\n",
    "        scores = regressor.evaluate(input_fn=test_input_fn)\n",
    "        print('MSE (tensorflow): {0:f}'.format(scores['loss']))\n",
    "\n",
    "        # regressor.fit(input_fn=lambda: input_fn(X_train, Y_train), steps=100)\n",
    "        # evaluation = regressor.evaluate(input_fn=lambda: input_fn(X_test), steps=1)\n",
    "        # loss_score = evaluation[\"loss\"]\n",
    "        # print(\"Final Loss on the testing set: {0:f}\".format(loss_score))\n",
    "        # y = regressor.predict(input_fn=lambda: input_fn(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x_data, y_data, n_features=50, n_diseases=1, n_epochs=250, n_batches=1000):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size=0.20)\n",
    "    training_size = X_train.shape[0]\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        x = tf.placeholder(tf.float32, shape=[None, n_features])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, n_diseases])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        neurons = {  # TODO train This new architecture\n",
    "            'in': n_features,\n",
    "            'l1': 1024,\n",
    "            'l2': 256,\n",
    "            'l3': 64,\n",
    "            'out': n_diseases\n",
    "        }\n",
    "        weights = {\n",
    "            'l1': weight_initializer(shape=[neurons['in'], neurons['l1']], stddev=0.1, name='w1'),\n",
    "            'l2': weight_initializer(shape=[neurons['l1'], neurons['l2']], stddev=0.1, name='w2'),\n",
    "            'l3': weight_initializer(shape=[neurons['l2'], neurons['l3']], stddev=0.1, name='w3'),\n",
    "            'out': weight_initializer(shape=[neurons['l3'], neurons['out']], stddev=0.1, name='w_out')\n",
    "        }\n",
    "\n",
    "        biases = {\n",
    "            'l1': bias_initializer(init_value=0.1, shape=[neurons['l1']], name='b1'),\n",
    "            'l2': bias_initializer(init_value=0.1, shape=[neurons['l2']], name='b2'),\n",
    "            'l3': bias_initializer(init_value=0.1, shape=[neurons['l3']], name='b3'),\n",
    "            'out': bias_initializer(init_value=0.1, shape=[neurons['out']], name='b_out')\n",
    "        }\n",
    "        # 1st Layer --> Fully Connected (1024 Neurons)\n",
    "        layer_1 = fully_connected(x, weights['l1'], biases['l1'], name='l1')\n",
    "        layer_1 = drop_out(layer_1, keep_prob)\n",
    "\n",
    "        # 2nd Layer --> Fully Connected (256 Neurons)\n",
    "        layer_2 = fully_connected(layer_1, weights['l2'], biases['l2'], name='l2')\n",
    "        layer_2 = drop_out(layer_2, keep_prob)\n",
    "\n",
    "        # 3rd Layer --> Fully Connected (64 Neurons)\n",
    "        layer_3 = fully_connected(layer_2, weights['l3'], biases['l3'], name='l3')\n",
    "        layer_3 = drop_out(layer_3, keep_prob)\n",
    "\n",
    "        # Final Layer --> Fully Connected (N_DISEASES Neurons)\n",
    "        final_output = fully_connected(layer_3, weights['out'], biases['out'], name='l_out')\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(final_output - y),name='loss')\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001, name='optimizer')\n",
    "        train_step = optimizer.minimize(loss, name='train_step')\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        training_acc = []\n",
    "        validation_acc = []\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            for epoch in range(n_epochs):\n",
    "                # Train Network\n",
    "                for i in range(10):\n",
    "                    batch_indices = np.random.choice(training_size, size=n_batches)\n",
    "                    x_train_batch = X_train.iloc[batch_indices]\n",
    "                    y_train_batch = Y_train.iloc[batch_indices]\n",
    "\n",
    "                    feed_dict = {x: x_train_batch, y: y_train_batch, keep_prob: DROP_OUT}\n",
    "                    _, train_loss = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "                    training_loss.append(train_loss)\n",
    "                    prediction = tf.nn.softmax(final_output)\n",
    "                    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_train_batch, 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                    feed_dict = {x: x_train_batch, y: y_train_batch, keep_prob: 1.0}\n",
    "                    training_acc.append(accuracy.eval(feed_dict))\n",
    "\n",
    "                    # Test Validation set\n",
    "                    feed_dict = {x: X_test, y: Y_test, keep_prob: 1.0}\n",
    "                    validation_loss.append(sess.run(loss, feed_dict=feed_dict))\n",
    "                    prediction = tf.nn.softmax(final_output)\n",
    "                    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y_test, 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                    validation_acc.append(accuracy.eval(feed_dict))\n",
    "                print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "                      \"\\tValidation Accuracy =\", '%01.9f' % (validation_acc[-1]),\n",
    "                      \"\\tValidation Loss =\", '%09.5f' % (validation_loss[-1]),\n",
    "                      \"\\tTraining Accuracy =\", '%01.9f' % (training_acc[-1]),\n",
    "                      \"\\tTraining Loss =\", '%09.5f' % (training_loss[-1]))\n",
    "        print(\"Training Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1696 15838  9315 13654 10717  7946 15729  8678 11177  5029  7296 14632\n",
      "  7613  8148  9387  8002  9141 18491 13226 18686 15959 11305 19531  5153\n",
      " 17493 14716  5082  4270 17034 18941  7755 19666  6542  5251 12374  9718\n",
      "  5573  3938 18681  1658 13259 10606 13480 19168  6729 18987  1586 12009\n",
      " 15312 19242]\n",
      "\n",
      "Loding Data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Users/Future/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas/_libs/parsers.c:10862)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas/_libs/parsers.c:11138)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows (pandas/_libs/parsers.c:12175)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data (pandas/_libs/parsers.c:14136)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas/_libs/parsers.c:14858)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas/_libs/parsers.c:15629)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/Future/anaconda/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8b67718d7942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-8b67718d7942>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# x_data = pd.DataFrame([[i for i in range(50)] for _ in range(100)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# y_data = pd.DataFrame([[i] for i in range(100)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOCAL_LOCATION_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOCAL_LOCATION_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Deep Neural Networks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7ca8212d341d>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO search for faster way to load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Future/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Future/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Future/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Future/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_features = np.random.choice(19671, 50, replace=False)\n",
    "print(np.array(random_features))\n",
    "print(\"\\nLoding Data...\")\n",
    "# x_data = pd.DataFrame([[i for i in range(50)] for _ in range(100)])\n",
    "# y_data = pd.DataFrame([[i] for i in range(100)])\n",
    "x_data, y_data = load_data(LOCAL_LOCATION_X)[random_features], load_data(LOCAL_LOCATION_Y)[0]\n",
    "print(y_data.shape)\n",
    "print(\"Train Deep Neural Networks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2(x_data, y_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
