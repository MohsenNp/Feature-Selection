{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Future/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "LOCAL_LOCATION_X = \"../Data/fpkm_normalized.csv\"\n",
    "LOCAL_LOCATION_Y = \"../Data/optimal_encoded_scae_dropout.csv\"\n",
    "\n",
    "# Hyper-Parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "DROP_OUT = 0.5\n",
    "N_SAMPLES = 10787\n",
    "N_FEATURES = 19671\n",
    "N_DISEASES = 34\n",
    "N_BATCHES = 2000\n",
    "N_EPOCHS = 100\n",
    "N_BATCH_LEARN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_initializer(shape, stddev=0.01, name=None):\n",
    "    initial = tf.random_normal(shape=shape, stddev=stddev)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def bias_initializer(shape, init_value=0.1, name=None):\n",
    "    initial = tf.constant(init_value, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def load_data(filename):  # TODO search for faster way to load data\n",
    "    return pd.read_csv(filename, header=None)\n",
    "\n",
    "\n",
    "def fully_connected(input_data, weight, bias, name=None):\n",
    "    return tf.nn.relu(tf.add(tf.matmul(input_data, weight), bias, name=name))\n",
    "\n",
    "\n",
    "def drop_out(prev_output, keep_prob):\n",
    "    return tf.nn.dropout(prev_output, keep_prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_data, y_data):\n",
    "    # Split data into train/test = 80%/20%\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size=0.20)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # training_size = X_train.shape[0]\n",
    "\n",
    "    # Create Network and Variables\n",
    "    with tf.Graph().as_default():\n",
    "        feature_columns = [tf.feature_column.numeric_column('x', shape=X_train.shape[1:])]\n",
    "        regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_columns,\n",
    "                                                  activation_fn=tf.nn.relu, hidden_units=[1024, 256, 64])\n",
    "\n",
    "        def input_fn(x, y=None):\n",
    "            if y is not None:  # Training\n",
    "                features = {k: tf.constant(x[k].values) for k in x.columns}\n",
    "                responses = tf.constant(y.values, shape=y.shape)\n",
    "                return features, responses\n",
    "            else:  # Testing\n",
    "                features = {k: tf.constant(x[k].values) for k in x_data.columns}\n",
    "                return features\n",
    "\n",
    "        # Training...\n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'x': X_train.values}, y=Y_train.values, batch_size=50, num_epochs=10000, shuffle=True)\n",
    "        print(\"Training...\")\n",
    "        regressor.fit(input_fn=train_input_fn, steps=5000)\n",
    "\n",
    "        print(\"Testing...\")\n",
    "        test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'x': X_test.values}, y=Y_test.values, num_epochs=1, shuffle=False)\n",
    "        predictions = regressor.predict_scores(input_fn=test_input_fn)\n",
    "\n",
    "        predictions = [p for p in predictions]\n",
    "        print(predictions)\n",
    "        print(len(predictions))\n",
    "        y_predicted = np.array(predictions)\n",
    "        y_predicted = y_predicted.reshape(np.array(Y_test).shape)\n",
    "\n",
    "        # Score with sklearn\n",
    "        score_sklearn = metrics.mean_squared_error(Y_test, y_predicted)\n",
    "        print('MSE (sklearn): {0:f}'.format(score_sklearn))\n",
    "\n",
    "        # Score with tensorflow\n",
    "        scores = regressor.evaluate(input_fn=test_input_fn)\n",
    "        print('MSE (tensorflow): {0:f}'.format(scores['loss']))\n",
    "\n",
    "        # regressor.fit(input_fn=lambda: input_fn(X_train, Y_train), steps=100)\n",
    "        # evaluation = regressor.evaluate(input_fn=lambda: input_fn(X_test), steps=1)\n",
    "        # loss_score = evaluation[\"loss\"]\n",
    "        # print(\"Final Loss on the testing set: {0:f}\".format(loss_score))\n",
    "        # y = regressor.predict(input_fn=lambda: input_fn(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x_data, y_data, n_features=50, n_diseases=1, n_epochs=250, n_batches=1000):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size=0.20)\n",
    "    training_size = X_train.shape[0]\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        x = tf.placeholder(tf.float32, shape=[None, n_features])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, n_diseases])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        neurons = {  # TODO train This new architecture\n",
    "            'in': n_features,\n",
    "            'l1': 1024,\n",
    "            'l2': 256,\n",
    "            'l3': 64,\n",
    "            'out': n_diseases\n",
    "        }\n",
    "        weights = {\n",
    "            'l1': weight_initializer(shape=[neurons['in'], neurons['l1']], stddev=0.1, name='w1'),\n",
    "            'l2': weight_initializer(shape=[neurons['l1'], neurons['l2']], stddev=0.1, name='w2'),\n",
    "            'l3': weight_initializer(shape=[neurons['l2'], neurons['l3']], stddev=0.1, name='w3'),\n",
    "            'out': weight_initializer(shape=[neurons['l3'], neurons['out']], stddev=0.1, name='w_out')\n",
    "        }\n",
    "\n",
    "        biases = {\n",
    "            'l1': bias_initializer(init_value=0.1, shape=[neurons['l1']], name='b1'),\n",
    "            'l2': bias_initializer(init_value=0.1, shape=[neurons['l2']], name='b2'),\n",
    "            'l3': bias_initializer(init_value=0.1, shape=[neurons['l3']], name='b3'),\n",
    "            'out': bias_initializer(init_value=0.1, shape=[neurons['out']], name='b_out')\n",
    "        }\n",
    "        # 1st Layer --> Fully Connected (1024 Neurons)\n",
    "        layer_1 = fully_connected(x, weights['l1'], biases['l1'], name='l1')\n",
    "        layer_1 = drop_out(layer_1, keep_prob)\n",
    "\n",
    "        # 2nd Layer --> Fully Connected (256 Neurons)\n",
    "        layer_2 = fully_connected(layer_1, weights['l2'], biases['l2'], name='l2')\n",
    "        layer_2 = drop_out(layer_2, keep_prob)\n",
    "\n",
    "        # 3rd Layer --> Fully Connected (64 Neurons)\n",
    "        layer_3 = fully_connected(layer_2, weights['l3'], biases['l3'], name='l3')\n",
    "        layer_3 = drop_out(layer_3, keep_prob)\n",
    "\n",
    "        # Final Layer --> Fully Connected (N_DISEASES Neurons)\n",
    "        final_output = fully_connected(layer_3, weights['out'], biases['out'], name='l_out')\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(final_output - y),name='loss')\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001, name='optimizer')\n",
    "        train_step = optimizer.minimize(loss, name='train_step')\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        training_acc = []\n",
    "        validation_acc = []\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            \n",
    "            for epoch in range(n_epochs):\n",
    "                # Train Network\n",
    "                for i in range(10):\n",
    "                    batch_indices = np.random.choice(training_size, size=n_batches)\n",
    "                    x_train_batch = X_train.iloc[batch_indices]\n",
    "                    y_train_batch = Y_train.iloc[batch_indices]\n",
    "\n",
    "                    feed_dict = {x: x_train_batch, y: y_train_batch, keep_prob: DROP_OUT}\n",
    "                    _, train_loss = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "                    training_loss.append(train_loss)\n",
    "                    prediction = tf.nn.softmax(final_output)\n",
    "                    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_train_batch, 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                    feed_dict = {x: x_train_batch, y: y_train_batch, keep_prob: 1.0}\n",
    "                    training_acc.append(accuracy.eval(feed_dict))\n",
    "\n",
    "                    # Test Validation set\n",
    "                    feed_dict = {x: X_test, y: Y_test, keep_prob: 1.0}\n",
    "                    validation_loss.append(sess.run(loss, feed_dict=feed_dict))\n",
    "                    prediction = tf.nn.softmax(final_output)\n",
    "                    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y_test, 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                    validation_acc.append(accuracy.eval(feed_dict))\n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "#                       \"\\tValidation Accuracy =\", '%01.9f' % (validation_acc[-1]),\n",
    "                      \"\\tValidation Loss =\", '%09.5f' % (validation_loss[-1]),\n",
    "#                       \"\\tTraining Accuracy =\", '%01.9f' % (training_acc[-1]),\n",
    "                      \"\\tTraining Loss =\", '%09.5f' % (training_loss[-1]))\n",
    "            \n",
    "        print(\"Training Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6754  7657 15877  9436 18254  4932  9344 14987  5105 15256 13168  4582\n",
      "  3842 17394  7482  8262  1403 12413  7309   274 15144  9325  1888 12452\n",
      "  7157  6334  9319  8696  3284  1485  5550  7127  1873  3417  5126 10667\n",
      "  3814  5438  8813 13760  5489 17299   331    98  5192  7813 16951 19005\n",
      " 13334 15886]\n",
      "\n",
      "Loding Data...\n",
      "(10787,)\n",
      "Train Deep Neural Networks\n"
     ]
    }
   ],
   "source": [
    "random_features = np.random.choice(19671, 50, replace=False)\n",
    "print(np.array(random_features))\n",
    "print(\"\\nLoding Data...\")\n",
    "# x_data = pd.DataFrame([[i for i in range(50)] for _ in range(100)])\n",
    "# y_data = pd.DataFrame([[i] for i in range(100)])\n",
    "x_data, y_data = load_data(LOCAL_LOCATION_X)[random_features], load_data(LOCAL_LOCATION_Y)[0]\n",
    "y_data = pd.DataFrame(np.reshape(y_data, newshape=[-1, 1]))\n",
    "print(y_data.shape)\n",
    "print(\"Train Deep Neural Networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'epoch' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-dc40114de537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-9a9ade443615>\u001b[0m in \u001b[0;36mmodel_2\u001b[0;34m(x_data, y_data, n_features, n_diseases, n_epochs, n_batches)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 print(\"Epoch:\", '%04d' % (epoch + 1),\n\u001b[1;32m     59\u001b[0m                       \u001b[0;34m\"\\tValidation Accuracy =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%01.9f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalidation_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'epoch' referenced before assignment"
     ]
    }
   ],
   "source": [
    "model_2(x_data, y_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
